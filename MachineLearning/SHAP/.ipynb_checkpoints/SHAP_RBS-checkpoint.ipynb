{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyORnT+JzrdoTWru7oUrOQyV"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"LW0H8RNF5mci"},"outputs":[],"source":["#!pip install shap==0.44.1\n","!pip install shap"]},{"cell_type":"code","source":["import shap\n","import pandas as pd\n","import numpy as np\n","shap.initjs()\n","\n"],"metadata":{"id":"rra5tTle5w6H"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["shap.__version__\n","customer = pd.DataFrame()\n","customer"],"metadata":{"id":"BC66jEfZSM7Y"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["customer = pd.read_csv(\"sample_data/CustomerChurn.csv\")\n","\n","customer.head()"],"metadata":{"id":"bipeQpdw6Lkh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(customer.dtypes)\n"],"metadata":{"id":"7XUCDELVYBHZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.metrics import classification_report\n","from sklearn.model_selection import train_test_split\n","\n","X = customer.drop(\"Churn\", axis='columns')\n","\n","\n","y = customer.Churn # Dependent variable\n","\n","# Split into train and test\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)\n","\n","# Train a machine learning model\n","from sklearn.ensemble import RandomForestClassifier\n","clf = RandomForestClassifier()\n","clf.fit(X_train, y_train)\n","\n","# Make prediction on the testing data\n","y_pred = clf.predict(X_test)\n","\n","# Classification Report\n","print(classification_report(y_pred, y_test))"],"metadata":{"id":"Ycg7hFXd65Fr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["y_pred\n"],"metadata":{"id":"vUF9Zeb9XgRJ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["This code performs a classification task using a Random Forest model.\n","\n","First, it separates the features (X) from the target variable (y).\n","The 'Churn' column is the target â€” it indicates whether a customer has left or not.\n","All other columns are used as input features to predict churn.\n","\n","The dataset is split into a training set (70%) and a test set (30%) using train_test_split.\n","The training set is used to train a RandomForestClassifier, a machine learning model that builds multiple decision trees.\n","Then, the model predicts the churn values for the test set (X_test).\n","\n","The classification_report function is used to print performance metrics comparing the predicted values (y_pred) with the actual values (y_test). The report includes precision, recall, F1-score, and support for each class.\n","\n","This process helps evaluate how well the model performs in predicting customer churn.\n"],"metadata":{"id":"gSH9RJDJ7Dqk"}},{"cell_type":"code","source":["# Crea TreeExplainer forzato\n","explainer = shap.Explainer(clf, X_train, feature_names=X_train.columns, model_output=\"probability\")\n","shap_values = explainer(X_test)\n","\n","# Verifica forme\n","print(\"shap_values shape:\", shap_values.values.shape)\n","print(\"X_test shape:\", X_test.shape)"],"metadata":{"id":"JNGLKlXWZnlE"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["This code uses SHAP (SHapley Additive exPlanations) to interpret the predictions made by the trained Random Forest model.\n","\n","A SHAP Explainer is created specifically for the trained model (clf) using the training data (X_train).\n","The parameter 'feature_names' ensures the explanations are labeled correctly.\n","'model_output=\"probability\"' tells SHAP to explain the model's predicted probabilities instead of raw scores or logits.\n","\n","Then, shap_values are calculated for the test set (X_test).\n","These values represent how much each feature contributed to pushing the prediction higher or lower for each individual test sample.\n","\n","Finally, the shapes of the resulting SHAP values and X_test are printed to confirm they match.\n","shap_values.values.shape should have dimensions (number of samples, number of features, number of classes),\n","and X_test.shape should match the sample and feature count.\n"],"metadata":{"id":"HIBHiBZ0anZP"}},{"cell_type":"code","source":["# Summary plot per la classe positiva (Churn = 1)\n","shap.summary_plot(shap_values.values[:, :, 1], X_test)"],"metadata":{"id":"ZsmMJktrZ4bF"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["This SHAP summary plot explains how each feature (column in your dataset) influenced the modelâ€™s prediction for Churn = 1 (meaning: the customer leaves).\n","Each dot is one customer. The x-axis shows how much a feature pushed the prediction towards or away from churn.\n","Features are listed from top to bottom by importance (top = most important).\n","Left side (negative SHAP value) = feature pushed the prediction toward no churn.\n","Right side (positive SHAP value) = feature pushed the prediction toward churn.\n","\n","Color shows the value of the feature for that customer:\n","ðŸ”´ Red = high value\n","ðŸ”µ Blue = low value\n","Status is the most important:\n","Red dots are on the right â†’ high \"Status\" values tend to increase churn risk.\n","Blue dots are on the left â†’ low \"Status\" values reduce churn risk.\n","Complains:\n","High values (red) strongly push predictions to churn.\n","This makes sense: customers who complain are more likely to leave.\n","Seconds of Use:\n","It has both red and blue spread on both sides.\n","This means the effect depends on the context (sometimes high usage reduces churn, sometimes it increases it)."],"metadata":{"id":"jxxD7Lg7aLDp"}},{"cell_type":"code","source":["#SHAP feature importance bar chart,\n","shap.summary_plot(shap_values.values[:, :, 1], X_test, plot_type=\"bar\")\n"],"metadata":{"id":"F-wAoOi1ahfE"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["SHAP feature importance bar chart,\n","This is a SHAP feature importance bar chart, showing the average impact of each feature on the modelâ€™s prediction for customer churn.\n","Each bar represents a feature from your dataset.\n","The longer the bar, the more influence that feature has on the modelâ€™s predictions.\n","The x-axis is the mean of the absolute SHAP values (mean(|SHAP value|)), which means:\n","How strongly, on average, a feature contributes to pushing a prediction toward either \"churn\" or \"not churn\" â€” regardless of direction."],"metadata":{"id":"PY6ZnytFa-eL"}},{"cell_type":"code","source":["#SHAP Decision Plot\n","shap.decision_plot(\n","    base_value=explainer.expected_value[1], # Use index 1 for the probability of class 1 (Churn)\n","    shap_values=shap_values.values[:5, :, 1], # Use index 1 for the probability of class 1 and select first 5 samples\n","    features=X_test.iloc[:5], # Select the first 5 samples from X_test\n","    feature_names=X_test.columns.tolist()\n",")\n"],"metadata":{"id":"ZmIEQwg8bOQn"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["This SHAP Decision Plot shows how the model made its predictions for the first 5 customers in your test set.\n","Each line represents one customer and how the model's output value (probability of churn) was built up step by step from the base value by adding the effect of each feature.\n","The x-axis shows the model output (probability of churn).\n","The left side (negative values) pushes toward no churn.\n","The right side (positive values) pushes toward churn.\n","The plot starts at the base value (average model output for the training set).\n","As you move from top to bottom, each feature's SHAP value is added or subtracted, modifying the prediction.\n","\n","The color of the lines shows the prediction:\n","ðŸ”´ Red = high churn risk\n","ðŸ”µ Blue = low churn risk\n","ðŸŸ£ Purple = in-between (medium risk)"],"metadata":{"id":"-Z-i7O3mbnEV"}},{"cell_type":"code","source":[],"metadata":{"id":"GCXF4TpcblhB"},"execution_count":null,"outputs":[]}]}